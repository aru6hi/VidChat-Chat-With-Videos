{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc9bbe5",
   "metadata": {},
   "source": [
    "# Multimodal RAG with Multimodal Langchain\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf152afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports & Utilities\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "def load_json_file(path: str):\n",
    "    \"\"\"Load JSON content from a file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97e19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your-key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c04bfc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/d309s7c57k58fd3w2q9mnq6m0000gn/T/ipykernel_42886/2891377093.py:2: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
      "/var/folders/9k/d309s7c57k58fd3w2q9mnq6m0000gn/T/ipykernel_42886/2891377093.py:3: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 3: Embeddings + VectorStore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5d292a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a4ecb8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/d309s7c57k58fd3w2q9mnq6m0000gn/T/ipykernel_42886/1796114822.py:2: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "# %% Cell 5: LLM Inference Client\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "def run_lvml_inference(prompt: str, image_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Run LLM on the given prompt.\n",
    "    Optionally caption the image and prepend it to `prompt`.\n",
    "    \"\"\"\n",
    "    # Example caption hook:\n",
    "    # if image_path:\n",
    "    #     caption = your_image_captioner(image_path)\n",
    "    #     prompt = f\"{caption}\\n\\n{prompt}\"\n",
    "\n",
    "    msgs = [HumanMessage(content=prompt)]\n",
    "    response = chat(msgs)\n",
    "    return response.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bda73208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 6: Retrieval Function\n",
    "def retrieve_video_segment(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant video segment for `query`.\n",
    "    Returns metadata including video_path, frame_path, transcript, and mid_time_ms.\n",
    "    \"\"\"\n",
    "    docs = retriever_module.get_relevant_documents(query)\n",
    "    if not docs:\n",
    "        return {}\n",
    "    meta = docs[0].metadata\n",
    "    return {\n",
    "        \"video_path\":      meta.get(\"video_path\"),\n",
    "        \"extracted_frame\": meta.get(\"frame_path\"),\n",
    "        \"transcript\":      meta.get(\"transcript\"),\n",
    "        \"mid_time_ms\":     meta.get(\"start_time\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69fd4442-4b7b-45ec-a16d-45112e1650e6",
   "metadata": {
    "height": 266
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "# Utility loader\n",
    "\n",
    "def load_json_file(path: str):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# 1) Set your API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"your-key\"\n",
    "\n",
    "# 2) Create embeddings and vector store\n",
    "embeddings = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 3) Retriever module\n",
    "retriever_module = vectorstore.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# 4) Inference client\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.0)\n",
    "\n",
    "def run_lvml_inference(prompt: str, image_path: str) -> str:\n",
    "    \"\"\"Run LLM on the given prompt (optionally prepend an image caption).\"\"\"\n",
    "    msgs = [HumanMessage(content=prompt)]\n",
    "    resp = chat(msgs)\n",
    "    return resp.content\n",
    "\n",
    "# 5) Retrieval wrapper\n",
    "\n",
    "def retrieve_video_segment(query: str):\n",
    "    docs = retriever_module.get_relevant_documents(query)\n",
    "    meta = docs[0].metadata\n",
    "    return {\n",
    "        \"video_path\":       meta.get(\"video_path\"),\n",
    "        \"extracted_frame\":  meta.get(\"frame_path\"),\n",
    "        \"transcript\":       meta.get(\"transcript\"),\n",
    "        \"mid_time_ms\":      meta.get(\"start_time\")\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1772071",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Setup vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ed9d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell X: Configure your Chroma DB (replaces Lancedb host + table)\n",
    "# Path where Chroma will read/write its on-disk index\n",
    "PERSIST_DIR     = \"./shared_data/chroma_db\"  \n",
    "\n",
    "# Name of the “collection” inside Chroma (was your LanceDB table)\n",
    "COLLECTION_NAME = \"test_tbl\"  \n",
    "# If you want to fall back to a demo collection:\n",
    "# COLLECTION_NAME = \"demo_tbl\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR, \n",
    "    embedding_function=embeddings, \n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31236e",
   "metadata": {},
   "source": [
    "### Retrieval Module\n",
    "#### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12db1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99411d0",
   "metadata": {},
   "source": [
    "#### Create Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00bc9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1) “uri”  →  where Chroma persists its on-disk index\n",
    "PERSIST_DIR     = \"./shared_data/chroma_db\"\n",
    "# 2) “table_name”  →  Chroma’s collection_name\n",
    "COLLECTION_NAME = \"test_tbl\"\n",
    "\n",
    "# 3) Create the embedder\n",
    "embedder = OpenAIEmbeddings(openai_api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# 4) Initialize Chroma (replaces MultimodalLanceDB)\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embedder,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# 5) Create a retriever (defaults to similarity search under the hood)\n",
    "retriever_module = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 1}   # exactly the same “k=1” you had\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458d619",
   "metadata": {},
   "source": [
    "#### Invoke Retrieval with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31b216e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:\n",
      " The view is always amazing I didn't think I would do another spacewalk and to now have the chance to have done four more was just icing on the cake for a a wonderful mission. Does the 10th one feel like the first one? No, a little more comfortable on the tenth one.\n",
      "Frame path: ./shared_data/videos/video1/extracted_frame/frame_7.jpg\n"
     ]
    }
   ],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "data = retrieve_video_segment_and_metadata(\"What do the astronauts feel about their work?\")\n",
    "print(\"Transcript:\\n\", data[\"transcript\"])\n",
    "print(\"Frame path:\", data[\"frame_path\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec3e85",
   "metadata": {},
   "source": [
    "### LVLM Inference Module\n",
    "\n",
    "#### Initialize Client and LVLM for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33dd921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: alias the wrapper as your LVLM module\n",
    "lvlm_inference_module = run_lvml_inference\n",
    "\n",
    "# Option B: alias the raw ChatOpenAI client (less convenient, since it expects HumanMessage lists)\n",
    "lvlm_inference_module = chat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286d225",
   "metadata": {},
   "source": [
    "#### Invoke LVLM Inference with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d2bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented query is:\n",
      " The transcript associated with the image is 'The view is always amazing I didn't think I would do another spacewalk and to now have the chance to have done four more was just icing on the cake for a a wonderful mission. Does the 10th one feel like the first one? No, a little more comfortable on the tenth one.'. What do the astronauts feel about their work?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/d309s7c57k58fd3w2q9mnq6m0000gn/T/ipykernel_21221/86350923.py:50: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  resp = chat(msgs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer:\n",
      " The astronauts feel amazed by their work and view from space. They also feel comfortable and experienced, especially after multiple spacewalks. They consider additional spacewalks as a bonus to their already wonderful mission.\n"
     ]
    }
   ],
   "source": [
    "# %% Cell X: Augment the user query with the retrieved transcript\n",
    "# `data` comes from your retrieve_video_segment_and_metadata call\n",
    "# `query` is the original user question\n",
    "\n",
    "augmented_query = (\n",
    "    f\"The transcript associated with the image is '{data['transcript']}'. \"\n",
    "    f\"{query}\"\n",
    ")\n",
    "print(\"Augmented query is:\\n\", augmented_query)\n",
    "\n",
    "# Now run your “LVLM” (ChatOpenAI wrapper) on this augmented prompt:\n",
    "answer = run_lvml_inference(\n",
    "    prompt=augmented_query,\n",
    "    image_path=data[\"frame_path\"]\n",
    ")\n",
    "print(\"\\nAnswer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "04136236-fbdf-4c53-aa4a-7a5e0e3d5de4",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LVLM Response:\n",
      "The astronauts feel amazed by their work and consider it a wonderful mission. They also feel more comfortable with their tasks over time, as indicated by the comparison between their first and tenth spacewalks.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# New code using our ChatOpenAI wrapper:\n",
    "response = run_lvml_inference(\n",
    "    prompt=augmented_query,\n",
    "    image_path=data[\"frame_path\"]   # same as your old `frame_path`\n",
    ")\n",
    "\n",
    "print('LVLM Response:')\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650be83a",
   "metadata": {},
   "source": [
    "### Prompt Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b2ea9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_processing_module(retrieved_results: list, user_query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of retrieved Document objects and the original query,\n",
    "    construct the prompt and return its text & image path.\n",
    "    \"\"\"\n",
    "    # Take first result\n",
    "    doc = retrieved_results[0]\n",
    "    meta = doc.metadata\n",
    "\n",
    "    transcript = meta.get(\"transcript\")\n",
    "    frame_path = meta.get(\"extracted_frame\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"The transcript associated with the image is '{transcript}'. \"\n",
    "        f\"{user_query}\"\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"image\": frame_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f31c2d",
   "metadata": {},
   "source": [
    "#### Invoke Prompt Processing Module with Retrieved Results and User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32f71f",
   "metadata": {},
   "source": [
    "### Multimodal RAG\n",
    "\n",
    "#### Define Multimodal RAG System as a Chain in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ccff79b-500f-4685-9ee8-6b3801f5a145",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def mm_rag_chain(user_query: str) -> str:\n",
    "    \"\"\"\n",
    "    End-to-end chain: retrieve document, process prompt, and run inference.\n",
    "    Returns the final answer string.\n",
    "    \"\"\"\n",
    "    # 1) Retrieve top-1 document\n",
    "    docs = retriever_module.get_relevant_documents(user_query)\n",
    "    if not docs:\n",
    "        return \"No relevant document found.\"\n",
    "\n",
    "    # 2) Build prompt & extract image path\n",
    "    processed = prompt_processing_module(docs, user_query)\n",
    "\n",
    "    # 3) Run LLM inference\n",
    "    answer = run_lvml_inference(\n",
    "        prompt=processed['prompt'],\n",
    "        image_path=processed['image']\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b614b1",
   "metadata": {},
   "source": [
    "***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
