{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bc9bbe5",
   "metadata": {},
   "source": [
    "# Multimodal RAG with Multimodal Langchain\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf152afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 1: Imports & Utilities\n",
    "\n",
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "def load_json_file(path: str):\n",
    "    \"\"\"Load JSON content from a file.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c97e19c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 2: API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"your-key\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c04bfc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 3: Embeddings + VectorStore\n",
    "embeddings = OpenAIEmbeddings(openai_api_key='YOUR-KEY')\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    "    embedding_function=embeddings\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5d292a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 4: Retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1a4ecb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% Cell 5: LLM Inference Client\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"gpt-4\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key='YOUR-KEY'\n",
    ")\n",
    "\n",
    "def run_lvml_inference(prompt: str, image_path: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Run LLM on the given prompt, including an image if a path is provided.\n",
    "    \"\"\"\n",
    "    content_parts = [{\"type\": \"text\", \"text\": prompt}]\n",
    "\n",
    "    if image_path and os.path.exists(image_path):\n",
    "        try:\n",
    "            # Read the image and encode it to base64\n",
    "            with Image.open(image_path) as img:\n",
    "                buffered = io.BytesIO()\n",
    "                img.save(buffered, format=\"JPEG\")\n",
    "                base64_image = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "                \n",
    "                content_parts.append({\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
    "                    }\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing image {image_path}: {e}\")\n",
    "    \n",
    "    msgs = [HumanMessage(content=content_parts)]\n",
    "    response = llm(msgs)\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bda73208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_video_segment(query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant video segment for `query`.\n",
    "    Returns metadata including video_path, frame_path, transcript, and mid_time_ms.\n",
    "    \"\"\"\n",
    "    docs = retriever_module.get_relevant_documents(query)\n",
    "    if not docs:\n",
    "        return {}\n",
    "    meta = docs[0].metadata\n",
    "    return {\n",
    "        \"video_path\":      meta.get(\"video_path\"),\n",
    "        \"extracted_frame\": meta.get(\"frame_path\"),\n",
    "        \"transcript\":      meta.get(\"transcript\"),\n",
    "        \"mid_time_ms\":     meta.get(\"start_time\"),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1772071",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "#### Setup vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ed9d593",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Path where Chroma will read/write its on-disk index\n",
    "PERSIST_DIR     = \"./shared_data/chroma_db\"\n",
    "\n",
    "# Name of the “collection” inside Chroma (was your LanceDB table)\n",
    "COLLECTION_NAME = \"test_tbl\"\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embeddings,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d31236e",
   "metadata": {},
   "source": [
    "### Retrieval Module\n",
    "#### Initialize Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12db1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key='YOUR-KEY'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99411d0",
   "metadata": {},
   "source": [
    "#### Create Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00bc9529",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1) “uri”  →  where Chroma persists its on-disk index\n",
    "PERSIST_DIR     = \"./shared_data/chroma_db\"\n",
    "# 2) “table_name”  →  Chroma’s collection_name\n",
    "COLLECTION_NAME = \"test_tbl\"\n",
    "\n",
    "# 3) Create the embedder\n",
    "embedder = OpenAIEmbeddings(openai_api_key='YOUR-KEY')\n",
    "\n",
    "# 4) Initialize Chroma\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=PERSIST_DIR,\n",
    "    embedding_function=embedder,\n",
    "    collection_name=COLLECTION_NAME\n",
    ")\n",
    "\n",
    "# 5) Create a retriever (defaults to similarity search under the hood)\n",
    "retriever_module = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 1}   # exactly the same “k=1” you had\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458d619",
   "metadata": {},
   "source": [
    "#### Invoke Retrieval with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "31b216e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:\n",
      " None\n",
      "Frame path: None\n"
     ]
    }
   ],
   "source": [
    "query = \"What do the astronauts feel about their work?\" # Define the query here\n",
    "data = retrieve_video_segment(query) # Corrected function call\n",
    "if data: # Check if data was retrieved\n",
    "    print(\"Transcript:\\n\", data[\"transcript\"])\n",
    "    print(\"Frame path:\", data[\"extracted_frame\"])\n",
    "else:\n",
    "    print(\"No relevant video segment found for the query.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ec3e85",
   "metadata": {},
   "source": [
    "### LVLM Inference Module\n",
    "\n",
    "#### Initialize Client and LVLM for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "33dd921d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alias the wrapper as your LVLM module\n",
    "lvlm_inference_module = run_lvml_inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3286d225",
   "metadata": {},
   "source": [
    "#### Invoke LVLM Inference with User Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "99d2bd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented query is:\n",
      " The transcript associated with the image is 'None'. What do the astronauts feel about their work?\n",
      "\n",
      "Answer:\n",
      " As an AI, I'm unable to view images or infer emotions. Therefore, I can't provide information about what the astronauts feel about their work based on an image.\n"
     ]
    }
   ],
   "source": [
    "if data: # Only proceed if data was successfully retrieved\n",
    "    augmented_query = (\n",
    "        f\"The transcript associated with the image is '{data['transcript']}'. \"\n",
    "        f\"{query}\"\n",
    "    )\n",
    "    print(\"Augmented query is:\\n\", augmented_query)\n",
    "\n",
    "    # Now run your “LVLM” (ChatOpenAI wrapper) on this augmented prompt:\n",
    "    answer = run_lvml_inference(\n",
    "        prompt=augmented_query,\n",
    "        image_path=data[\"extracted_frame\"] # Changed to 'extracted_frame' for consistency with `retrieve_video_segment`\n",
    "    )\n",
    "    print(\"\\nAnswer:\\n\", answer)\n",
    "else:\n",
    "    print(\"Cannot run LVLM inference: No data was retrieved in the previous step.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "04136236-fbdf-4c53-aa4a-7a5e0e3d5de4",
   "metadata": {
    "height": 149
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LVLM Response:\n",
      "As an AI, I'm unable to provide an analysis without specific text, audio, or visual data. Please provide more information.\n"
     ]
    }
   ],
   "source": [
    "if data and 'augmented_query' in locals(): # Check if data and augmented_query exist\n",
    "    response = run_lvml_inference(\n",
    "        prompt=augmented_query,\n",
    "        image_path=data[\"extracted_frame\"] # Changed to 'extracted_frame'\n",
    "    )\n",
    "\n",
    "    print('LVLM Response:')\n",
    "    print(response)\n",
    "else:\n",
    "    print(\"Cannot run LVLM inference: `augmented_query` or `data` is not defined.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650be83a",
   "metadata": {},
   "source": [
    "### Prompt Processing Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b2ea9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_processing_module(retrieved_results: List, user_query: str) -> dict:\n",
    "    \"\"\"\n",
    "    Given a list of retrieved Document objects and the original query,\n",
    "    construct the prompt and return its text & image path.\n",
    "    \"\"\"\n",
    "    # Take the first result\n",
    "    doc = retrieved_results[0]\n",
    "    meta = doc.metadata\n",
    "\n",
    "    document_text = meta.get(\"transcript\") or doc.page_content\n",
    "    frame_path = meta.get(\"frame_path\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"The relevant document content is: '{document_text}'. \"\n",
    "        f\"Based on this, answer the following question: {user_query}\"\n",
    "    )\n",
    "    return {\"prompt\": prompt, \"image\": frame_path, \"document_text\": document_text}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f31c2d",
   "metadata": {},
   "source": [
    "#### Invoke Prompt Processing Module with Retrieved Results and User Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32f71f",
   "metadata": {},
   "source": [
    "### Multimodal RAG\n",
    "\n",
    "#### Define Multimodal RAG System as a Chain in LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ccff79b-500f-4685-9ee8-6b3801f5a145",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "def mm_rag_chain(user_query: str) -> tuple[str, str, str]:\n",
    "    \"\"\"\n",
    "    End-to-end chain: retrieve document, process prompt, and run inference.\n",
    "    Returns the final answer string, the image path, and the extracted text.\n",
    "    \"\"\"\n",
    "    global VECTOR_STORE\n",
    "    \n",
    "    if VECTOR_STORE is None:\n",
    "        return \"Error: RAG system is not initialized. Please check the setup.\", None, None\n",
    "    \n",
    "    # Create a retriever from the vector store\n",
    "    retriever_module = VECTOR_STORE.as_retriever(search_kwargs={\"k\": 1})\n",
    "    \n",
    "    # 1) Retrieve top-1 document\n",
    "    docs = retriever_module.get_relevant_documents(user_query)\n",
    "    if not docs:\n",
    "        return \"No relevant document found.\", None, None\n",
    "\n",
    "    # 2) Build prompt & extract image path\n",
    "    processed = prompt_processing_module(docs, user_query)\n",
    "    prompt = processed['prompt']\n",
    "    image_path = processed['image']\n",
    "    document_text = processed['document_text']\n",
    "\n",
    "    # --- Debugging print statement added here ---\n",
    "    print(f\"Retrieved image path: {image_path}\")\n",
    "\n",
    "    # 3) Run LLM inference\n",
    "    answer = run_lvml_inference(\n",
    "        prompt=prompt,\n",
    "        image_path=image_path\n",
    "    )\n",
    "    return answer, image_path, document_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b614b1",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50da7897",
   "metadata": {},
   "source": [
    "# Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "05e58baf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9k/d309s7c57k58fd3w2q9mnq6m0000gn/T/ipykernel_21488/762685702.py:93: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7871\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7871/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved image path: None\n",
      "Retrieved image path: None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gradio as gr\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# --- Global Configuration and Setup ---\n",
    "\n",
    "# Set up your OpenAI API key\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR-KEY\" \n",
    "\n",
    "# Initialize LangChain components\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-4o-mini\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Path to the Chroma database (assuming it's already populated with your data)\n",
    "PERSIST_DIR = \"./shared_data/chroma_db\"\n",
    "COLLECTION_NAME = \"test_tbl\"\n",
    "\n",
    "# Global variable for the vector store\n",
    "VECTOR_STORE = None\n",
    "CHAT_HISTORY = []\n",
    "\n",
    "# --- Backend Functions ---\n",
    "\n",
    "def init_rag_system():\n",
    "    \"\"\"\n",
    "    Initializes the Chroma vector store from the persistent directory.\n",
    "    This function is called once when the app starts.\n",
    "    \"\"\"\n",
    "    global VECTOR_STORE\n",
    "    \n",
    "    try:\n",
    "        # Check if the directory exists\n",
    "        if not os.path.exists(PERSIST_DIR):\n",
    "            return f\"Error: The directory '{PERSIST_DIR}' was not found. Please ensure your ChromaDB is at this location.\"\n",
    "        \n",
    "        # Load the Chroma vector store\n",
    "        VECTOR_STORE = Chroma(\n",
    "            persist_directory=PERSIST_DIR,\n",
    "            embedding_function=embeddings,\n",
    "            collection_name=COLLECTION_NAME\n",
    "        )\n",
    "        \n",
    "        return \"RAG system initialized successfully from local data.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred during RAG system initialization: {str(e)}\"\n",
    "\n",
    "def respond(message, chat_history):\n",
    "    \"\"\"\n",
    "    Responds to a user message using the multimodal RAG chain and returns the\n",
    "    response, image path, and extracted text.\n",
    "    \"\"\"\n",
    "    global CHAT_HISTORY\n",
    "    \n",
    "    # Call the mm_rag_chain with the user's message\n",
    "    response, image_path, extracted_text = mm_rag_chain(message)\n",
    "    \n",
    "    # Check if an image was retrieved and add it to the chat history\n",
    "    if image_path:\n",
    "        # Gradio chatbot can display images by using a tuple (text, image_path)\n",
    "        CHAT_HISTORY.append((message, (response, image_path)))\n",
    "    else:\n",
    "        CHAT_HISTORY.append((message, response))\n",
    "    \n",
    "    return \"\", CHAT_HISTORY, extracted_text\n",
    "\n",
    "# --- Gradio Interface ---\n",
    "\n",
    "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
    "    gr.Markdown(\n",
    "        \"\"\"\n",
    "        # Welcome to VidChat !\n",
    "        Ask questions based on your locally processed video.\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Status message to display RAG system initialization feedback\n",
    "    status_message = gr.Textbox(\n",
    "        label=\"Status\",\n",
    "        value=init_rag_system(),\n",
    "        interactive=False\n",
    "    )\n",
    "    \n",
    "    # Extracted text display\n",
    "    extracted_text_display = gr.Textbox(\n",
    "        label=\"Extracted Content\",\n",
    "        lines=5,\n",
    "        interactive=False\n",
    "    )\n",
    "    \n",
    "    # Chatbot interface\n",
    "    chatbot = gr.Chatbot(\n",
    "        label=\"Chat about your video\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    # User message input\n",
    "    msg = gr.Textbox(\n",
    "        label=\"Your Question\",\n",
    "        placeholder=\"e.g., What is the main topic of the content?\",\n",
    "        interactive=True\n",
    "    )\n",
    "    \n",
    "    # Action buttons\n",
    "    with gr.Row():\n",
    "        submit_button = gr.Button(\"Send\")\n",
    "        clear_button = gr.ClearButton([msg, chatbot, extracted_text_display])\n",
    "\n",
    "    # Event handlers\n",
    "    submit_button.click(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot, extracted_text_display]\n",
    "    )\n",
    "    \n",
    "    # Handle submitting with Enter key\n",
    "    msg.submit(\n",
    "        fn=respond,\n",
    "        inputs=[msg, chatbot],\n",
    "        outputs=[msg, chatbot, extracted_text_display]\n",
    "    )\n",
    "\n",
    "# Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    # The 'inline=True' parameter tells Gradio to display the app within the notebook\n",
    "    # instead of opening a new browser tab.\n",
    "    demo.launch(inline=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
